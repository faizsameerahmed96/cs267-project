{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Chunking\n",
    "- We will be using a basic text splitter for chunks.\n",
    "- We will be ignoring images and table data in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# To get poppler working on MacOS\n",
    "os.environ['PATH'] += os.pathsep + '/opt/homebrew/opt/poppler/bin:/Users/faizahmed/.pyenv/versions/3.11.8/bin:/opt/miniconda3/condabin:/Users/faizahmed/Library/pnpm:/Users/faizahmed/.pyenv/shims:/Users/faizahmed/.rbenv/shims:/Users/faizahmed/.nvm/versions/node/v18.16.1/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Library/TeX/texbin:/Users/faizahmed/.cargo/bin:/Users/faizahmed/.pyenv/versions/3.11.8/bin:/Users/faizahmed/Library/Android/sdk/emulator:/Users/faizahmed/Library/Android/sdk/platform-tools:/opt/homebrew/Cellar/hadoop/3.4.1/libexec/bin:/opt/homebrew/Cellar/hadoop/3.4.1/libexec/sbin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Vector Database to store vectors\n",
    "We will be using Qdrant as our vector database. It supports vector search along with metadata search which makes it convenient for usage. Also open source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install qdrant_client\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "\n",
    "# Testing if the client is working\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"test_collection\",\n",
    "    vectors_config=VectorParams(size=4, distance=Distance.DOT),\n",
    ")\n",
    "\n",
    "client.delete_collection(collection_name=\"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Langchain and OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai\n",
    "MAX_CHUNK_SIZE = 1024\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", chunk_size=MAX_CHUNK_SIZE)\n",
    "\n",
    "# Testing if the embeddings model is working\n",
    "# embeddings_model.embed_documents([\"Hello, world!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Chunks from PDF files\n",
    "\n",
    "In this exploration, we will use 1024 size chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf, Element\n",
    "\n",
    "def generate_elements(file_path) -> list[Element]:\n",
    "    elements = partition_pdf(\n",
    "            filename=file_path,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=MAX_CHUNK_SIZE,\n",
    "            # Unstructured Helpers\n",
    "            strategy=\"auto\", \n",
    "        #     infer_table_structure=True, \n",
    "            # model_name=\"yolox\",\n",
    "            # extract_images_in_pdf=True,\n",
    "            # image_output_dir_path=\"static/pdfImages/\"\n",
    "    )\n",
    "\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../../research_papers/mapreduce.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = generate_elements(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'text': 'MapReduce: Simplified Data Processing on Large Clusters\\\\n\\\\nby Jeffrey Dean and Sanjay Ghemawat\\\\n\\\\nAbstract\\\\n\\\\nMapReduce is a programming model and an associated implementation for processing\\\\n\\\\nand generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the under- lying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make effi- cient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Googleâ€™s clusters every day, processing a total of more than twenty petabytes of data per day.', 'embeddings': None, '_element_id': '0d109a749dae3ebd8a7d171fe709e36d', 'metadata': <unstructured.documents.elements.ElementMetadata object at 0x33f078cd0>}\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements[0].metadata.page_number\n",
    "elements[0].text\n",
    "str(elements[0].__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings and store to qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings_model.embed_documents([element.text for element in elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not client.collection_exists(\"pdf_collection\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"pdf_collection\",\n",
    "        vectors_config=VectorParams(size=len(embeddings[0]), distance=Distance.COSINE),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=1, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate uuid\n",
    "import uuid\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "points = []\n",
    "for i, element in enumerate(elements[:]):\n",
    "    point = PointStruct(\n",
    "        id=str(uuid.uuid4()),\n",
    "        vector=embeddings[i],\n",
    "        payload={\"text\": element.text, \"page\": element.metadata.page_number}\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "client.upsert(collection_name=\"pdf_collection\", points=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"execution steps of map reduce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = embeddings_model.embed_documents([question])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='6a899300-eac3-4ca4-a201-04c1cf7f1bca', version=1, score=0.6226835, payload={'text': '3.1 Execution Overview\\n\\nThe map invocations are distributed across multiple machines by auto- matically partitioning the input data into a set of M splits. The input splits can be processed in parallel by different machines. Reduce invo- cations are distributed by partitioning the intermediate key space into R pieces using a partitioning function (e.g., hash(key) mod R). The number of partitions (R) and the partitioning function are specified by the user.\\n\\nFigure 1 shows the overall flow of a MapReduce operation in our implementation. When the user program calls the MapReduce func- tion, the following sequence of actions occurs (the numbered labels in Figure 1 correspond to the numbers in the following list).\\n\\n1. The MapReduce library in the user program first splits the input files into M pieces of typically 16-64MB per piece (controllable by the user via an optional parameter). It then starts up many copies of the program on a cluster of machines.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='e83b17a3-cdd1-4828-9b0a-e4aef16d8177', version=0, score=0.6226226, payload={'text': '3.1 Execution Overview\\n\\nThe map invocations are distributed across multiple machines by auto- matically partitioning the input data into a set of M splits. The input splits can be processed in parallel by different machines. Reduce invo- cations are distributed by partitioning the intermediate key space into R pieces using a partitioning function (e.g., hash(key) mod R). The number of partitions (R) and the partitioning function are specified by the user.\\n\\nFigure 1 shows the overall flow of a MapReduce operation in our implementation. When the user program calls the MapReduce func- tion, the following sequence of actions occurs (the numbered labels in Figure 1 correspond to the numbers in the following list).\\n\\n1. The MapReduce library in the user program first splits the input files into M pieces of typically 16-64MB per piece (controllable by the user via an optional parameter). It then starts up many copies of the program on a cluster of machines.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='3dfb2a93-6684-4b89-9be4-0e020cc07e52', version=1, score=0.6138454, payload={'text': 'To ECE\\n\\nMapReduce: Simplified Data Processing on Large Clusters\\n\\nUser  Program  (1) fork  (1) fork  (1) fork  Master  (2)  (2)  assign  assign  map  reduce  worker  split 0  split 1  split 2  (3) read  worker  (4) local write     ( 5 )  ( 5 )  r     e m o t  e a d  r  e    worker  (6) write  output  file 0  split 3  split 4  worker  output  file 1  worker  Input  Map  Intermediate files  Reduce  Output  files  phasr  (on local disks)  phase  files \\n\\nFig. 1. Execution overview.\\n\\n7. When all map tasks and reduce tasks have been completed, the mas- ter wakes up the user program. At this point, the MapReduce call in the user program returns back to the user code.', 'page': 3}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='6d40d5a0-df79-4c2b-9bc7-fa439da4bf41', version=0, score=0.61377513, payload={'text': 'To ECE\\n\\nMapReduce: Simplified Data Processing on Large Clusters\\n\\nUser  Program  (1) fork  (1) fork  (1) fork  Master  (2)  (2)  assign  assign  map  reduce  worker  split 0  split 1  split 2  (3) read  worker  (4) local write     ( 5 )  ( 5 )  r     e m o t  e a d  r  e    worker  (6) write  output  file 0  split 3  split 4  worker  output  file 1  worker  Input  Map  Intermediate files  Reduce  Output  files  phasr  (on local disks)  phase  files \\n\\nFig. 1. Execution overview.\\n\\n7. When all map tasks and reduce tasks have been completed, the mas- ter wakes up the user program. At this point, the MapReduce call in the user program returns back to the user code.', 'page': 3}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='4ef637da-819b-4d10-9280-8b7d785a585f', version=1, score=0.59493315, payload={'text': '2. One of the copies of the programâ€”the masterâ€” is special. The rest are workers that are assigned work by the master. There are M map tasks and R reduce tasks to assign. The master picks idle workers and assigns each one a map task or a reduce task.\\n\\n3. A worker who is assigned a map task reads the contents of the corre- sponding input split. It parses key/value pairs out of the input data and passes each pair to the user-defined map function. The intermediate key/value pairs produced by the map function are buffered in memory.\\n\\n4. Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master who is responsible for forwarding these locations to the reduce workers.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='fed7f15b-3b14-46c6-8a27-1c6d356f8a4f', version=0, score=0.59490424, payload={'text': '2. One of the copies of the programâ€”the masterâ€” is special. The rest are workers that are assigned work by the master. There are M map tasks and R reduce tasks to assign. The master picks idle workers and assigns each one a map task or a reduce task.\\n\\n3. A worker who is assigned a map task reads the contents of the corre- sponding input split. It parses key/value pairs out of the input data and passes each pair to the user-defined map function. The intermediate key/value pairs produced by the map function are buffered in memory.\\n\\n4. Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master who is responsible for forwarding these locations to the reduce workers.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='6106c98b-478c-457f-851b-f3e79299ba1d', version=0, score=0.5789184, payload={'text': '5. When a reduce worker is notified by the master about these loca- tions, it uses remote procedure calls to read the buffered data from the local disks of the map workers. When a reduce worker has read all intermediate data for its partition, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together. The sorting is needed because typically many different keys map to the same reduce task. If the amount of intermediate data is too large to fit in memory, an external sort is used.\\n\\n6. The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the userâ€™s reduce func- tion. The output of the reduce function is appended to a final out- put file for this reduce partition.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='c937ca61-9480-4907-bf0e-a9b2a238fec5', version=1, score=0.5787483, payload={'text': '5. When a reduce worker is notified by the master about these loca- tions, it uses remote procedure calls to read the buffered data from the local disks of the map workers. When a reduce worker has read all intermediate data for its partition, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together. The sorting is needed because typically many different keys map to the same reduce task. If the amount of intermediate data is too large to fit in memory, an external sort is used.\\n\\n6. The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the userâ€™s reduce func- tion. The output of the reduce function is appended to a final out- put file for this reduce partition.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='b2f63afa-d2c5-494c-ae1c-9d3b1118a0a3', version=1, score=0.566556, payload={'text': 'More than ten thousand distinct programs have been implemented using MapReduce at Google, including algorithms for large-scale graph processing, text processing, data mining, machine learning, sta- tistical machine translation, and many other areas. More discussion of specific applications of MapReduce can be found elsewhere [8, 16, 7].\\n\\n2.2 Types\\n\\nEven though the previous pseudocode is written in terms of string inputs and outputs, conceptually the map and reduce functions sup- plied by the user have associated types.\\n\\nmap (k1,v1) â†’ list(k2,v2) reduce (k2,list(v2)) â†’ list(v2)\\n\\nThat is, the input keys and values are drawn from a different domain than the output keys and values. Furthermore, the intermediate keys and values are from the same domain as the output keys and values.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='664a28ff-61db-432a-bf9e-732b407ccf60', version=0, score=0.5662421, payload={'text': 'More than ten thousand distinct programs have been implemented using MapReduce at Google, including algorithms for large-scale graph processing, text processing, data mining, machine learning, sta- tistical machine translation, and many other areas. More discussion of specific applications of MapReduce can be found elsewhere [8, 16, 7].\\n\\n2.2 Types\\n\\nEven though the previous pseudocode is written in terms of string inputs and outputs, conceptually the map and reduce functions sup- plied by the user have associated types.\\n\\nmap (k1,v1) â†’ list(k2,v2) reduce (k2,list(v2)) â†’ list(v2)\\n\\nThat is, the input keys and values are drawn from a different domain than the output keys and values. Furthermore, the intermediate keys and values are from the same domain as the output keys and values.', 'page': 2}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.search(collection_name=\"pdf_collection\", query_vector=question_embedding, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

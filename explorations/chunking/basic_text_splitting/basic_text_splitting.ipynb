{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Chunking\n",
    "- We will be using a basic text splitter for chunks.\n",
    "- We will be ignoring images and table data in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Vector Database to store vectors\n",
    "We will be using Qdrant as our vector database. It supports vector search along with metadata search which makes it convenient for usage. Also open source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install qdrant_client\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "\n",
    "# Testing if the client is working\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"test_collection\",\n",
    "    vectors_config=VectorParams(size=4, distance=Distance.DOT),\n",
    ")\n",
    "\n",
    "client.delete_collection(collection_name=\"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Langchain and OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai\n",
    "MAX_CHUNK_SIZE = 1024\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", chunk_size=MAX_CHUNK_SIZE)\n",
    "\n",
    "# Testing if the embeddings model is working\n",
    "# embeddings_model.embed_documents([\"Hello, world!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Chunks from PDF files\n",
    "\n",
    "In this exploration, we will use 1024 size chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizahmed/.pyenv/versions/3.11.8/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf, Element\n",
    "\n",
    "def generate_elements(file_path) -> list[Element]:\n",
    "    elements = partition_pdf(\n",
    "            filename=file_path,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=MAX_CHUNK_SIZE,\n",
    "            # Unstructured Helpers\n",
    "            strategy=\"auto\", \n",
    "            infer_table_structure=True, \n",
    "            model_name=\"yolox\",\n",
    "            extract_images_in_pdf=True,\n",
    "            image_output_dir_path=\"static/pdfImages/\"\n",
    "    )\n",
    "\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../../research_papers/mapreduce.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = generate_elements(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings and store to qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings_model.embed_documents([element.text for element in elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not client.collection_exists(\"pdf_collection\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"pdf_collection\",\n",
    "        vectors_config=VectorParams(size=len(embeddings[0]), distance=Distance.COSINE),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate uuid\n",
    "import uuid\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "points = []\n",
    "for i, element in enumerate(elements[:]):\n",
    "    point = PointStruct(\n",
    "        id=str(uuid.uuid4()),\n",
    "        vector=embeddings[i],\n",
    "        payload={\"text\": element.text, \"page\": element.metadata.page_number}\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "client.upsert(collection_name=\"pdf_collection\", points=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"execution steps of map reduce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = embeddings_model.embed_documents([question])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='e83b17a3-cdd1-4828-9b0a-e4aef16d8177', version=0, score=0.6226226, payload={'text': '3.1 Execution Overview\\n\\nThe map invocations are distributed across multiple machines by auto- matically partitioning the input data into a set of M splits. The input splits can be processed in parallel by different machines. Reduce invo- cations are distributed by partitioning the intermediate key space into R pieces using a partitioning function (e.g., hash(key) mod R). The number of partitions (R) and the partitioning function are specified by the user.\\n\\nFigure 1 shows the overall flow of a MapReduce operation in our implementation. When the user program calls the MapReduce func- tion, the following sequence of actions occurs (the numbered labels in Figure 1 correspond to the numbers in the following list).\\n\\n1. The MapReduce library in the user program first splits the input files into M pieces of typically 16-64MB per piece (controllable by the user via an optional parameter). It then starts up many copies of the program on a cluster of machines.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='6d40d5a0-df79-4c2b-9bc7-fa439da4bf41', version=0, score=0.61377513, payload={'text': 'To ECE\\n\\nMapReduce: Simplified Data Processing on Large Clusters\\n\\nUser  Program  (1) fork  (1) fork  (1) fork  Master  (2)  (2)  assign  assign  map  reduce  worker  split 0  split 1  split 2  (3) read  worker  (4) local write     ( 5 )  ( 5 )  r     e m o t  e a d  r  e    worker  (6) write  output  file 0  split 3  split 4  worker  output  file 1  worker  Input  Map  Intermediate files  Reduce  Output  files  phasr  (on local disks)  phase  files \\n\\nFig. 1. Execution overview.\\n\\n7. When all map tasks and reduce tasks have been completed, the mas- ter wakes up the user program. At this point, the MapReduce call in the user program returns back to the user code.', 'page': 3}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='fed7f15b-3b14-46c6-8a27-1c6d356f8a4f', version=0, score=0.59490424, payload={'text': '2. One of the copies of the program—the master— is special. The rest are workers that are assigned work by the master. There are M map tasks and R reduce tasks to assign. The master picks idle workers and assigns each one a map task or a reduce task.\\n\\n3. A worker who is assigned a map task reads the contents of the corre- sponding input split. It parses key/value pairs out of the input data and passes each pair to the user-defined map function. The intermediate key/value pairs produced by the map function are buffered in memory.\\n\\n4. Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master who is responsible for forwarding these locations to the reduce workers.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='6106c98b-478c-457f-851b-f3e79299ba1d', version=0, score=0.5789184, payload={'text': '5. When a reduce worker is notified by the master about these loca- tions, it uses remote procedure calls to read the buffered data from the local disks of the map workers. When a reduce worker has read all intermediate data for its partition, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together. The sorting is needed because typically many different keys map to the same reduce task. If the amount of intermediate data is too large to fit in memory, an external sort is used.\\n\\n6. The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the user’s reduce func- tion. The output of the reduce function is appended to a final out- put file for this reduce partition.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='664a28ff-61db-432a-bf9e-732b407ccf60', version=0, score=0.5662421, payload={'text': 'More than ten thousand distinct programs have been implemented using MapReduce at Google, including algorithms for large-scale graph processing, text processing, data mining, machine learning, sta- tistical machine translation, and many other areas. More discussion of specific applications of MapReduce can be found elsewhere [8, 16, 7].\\n\\n2.2 Types\\n\\nEven though the previous pseudocode is written in terms of string inputs and outputs, conceptually the map and reduce functions sup- plied by the user have associated types.\\n\\nmap (k1,v1) → list(k2,v2) reduce (k2,list(v2)) → list(v2)\\n\\nThat is, the input keys and values are drawn from a different domain than the output keys and values. Furthermore, the intermediate keys and values are from the same domain as the output keys and values.', 'page': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='4c685ac3-e17d-4e4f-b941-9c87d06f08f9', version=0, score=0.56272924, payload={'text': 'After successful completion, the output of the mapreduce execution is available in the R output files (one per reduce task, with file names specified by the user). Typically, users do not need to combine these R output files into one file; they often pass these files as input to another MapReduce call or use them from another distributed application that is able to deal with input that is partitioned into multiple files.', 'page': 3}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='f89424da-8031-43ab-b3cd-ae4916cca385', version=0, score=0.56161976, payload={'text': '2 Programming Model\\n\\nThe computation takes a set of input key/value pairs, and produces a set of output key/value pairs. The user of the MapReduce library expresses the computation as two functions: map and reduce.\\n\\nMap, written by the user, takes an input pair and produces a set of intermediate key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key I and passes them to the reduce function.\\n\\nThe reduce function, also written by the user, accepts an interme- diate key I and a set of values for that key. It merges these values together to form a possibly smaller set of values. Typically just zero or one output value is produced per reduce invocation. The intermediate values are supplied to the user’s reduce function via an iterator. This allows us to handle lists of values that are too large to fit in memory.', 'page': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='5b386f1d-93ec-4806-8131-272506063084', version=0, score=0.5504986, payload={'text': '4 Refinements\\n\\nAlthough the basic functionality provided by simply writing map and reduce functions is sufficient for most needs, we have found a few extensions useful. These include:\\n\\ne user-specified partitioning functions for determining the mapping of intermediate key values to the R reduce shards;\\n\\ne ordering guarantees: Our implementation guarantees that within each of the R reduce partitions, the intermediate key/value pairs are processed in increasing key order;\\n\\n© user-specified combiner functions for doing partial combination of generated intermediate values with the same key within the same map task (to reduce the amount of intermediate data that must be transferred across the network);\\n\\n© custom input and output types, for reading new input formats and producing new output formats;\\n\\n¢ amode for execution on a single machine for simplifying debugging and small-scale testing.\\n\\nThe original article has more detaile d discussions of each of these items [8].', 'page': 4}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='0bfeb5fb-4d60-4045-8ffe-b7ff5d9fad67', version=0, score=0.542714, payload={'text': 'e The indexing code is simpler, smaller, and easier to understand be- cause the code that deals with fault tolerance, distribution, and par- allelization is hidden within the MapReduce library. For example, the size of one phase of the computation dropped from approximately 3800 lines of C++ code to approximately 700 lines when expressed using MapReduce.\\n\\ne The performance of the MapReduce library is good enough that we can keep conceptually unrelated computations separate instead of mixing them together to avoid extra passes over the data. This makes it easy to change the indexing process. For example, one change that took a few months to make in our old indexing system took only a few days to implement in the new system.', 'page': 6}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='92866d18-81a3-45ab-936b-a470821c9c11', version=0, score=0.5407682, payload={'text': '1 Introduction\\n\\nPrior to our development of MapReduce, the authors and many others at Google implemented hundreds of special-purpose computations that process large amounts of raw data, such as crawled documents, Web request logs, etc., to compute various kinds of derived data, such as inverted indices, various representations of the graph structure of Web documents, summaries of the number of pages crawled per host, and the set of most frequent queries in a given day. Most such computa- tions are conceptually straightforward. However, the input data is usu- ally large and the computations have to be distributed across hundreds or thousands of machines in order to finish in a reasonable amount of time. The issues of how to parallelize the computation, distribute the data, and handle failures conspire to obscure the original simple com- putation with large amounts of complex code to deal with these issues.', 'page': 1}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.search(collection_name=\"pdf_collection\", query_vector=question_embedding, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

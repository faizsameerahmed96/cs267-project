{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Search \n",
    "\n",
    "<p>Start agentic search using Langchain. We will define a agent with the following tools:</p>\n",
    "\n",
    "- Ability to query our Qdrant database \n",
    "  \n",
    "  - Allow it to also make it's own queries and get the best query\n",
    "  \n",
    "- Ability to open our pdf and read a particular page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHUNK_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "\n",
    "from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", chunk_size=MAX_CHUNK_SIZE)\n",
    "client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"pdf_collection\",\n",
    "    embedding=embeddings_model,\n",
    ")\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def natural_language_query(input: str):\n",
    "    \"\"\" Will query the vector store that contains information about the research papers and return the top 10 most relevant chunks from all the papers.\n",
    "    Each result has the following structure:\n",
    "    {\n",
    "        \"text\": \"string\",\n",
    "        \"pdf_path\": \"string\", # the path to the pdf file\n",
    "        \"page_number\": 0, # the page number where the text is located\n",
    "        \"idx\": string # the pdf is chunked into smaller pieces, this is the id of the chunk. eg: 1.txt, 2.txt, etc. Each document is split sequentially into chunks.\n",
    "        \"score\": 0.0 # the cosine similarity between the input and the text\n",
    "    }\n",
    "    \"\"\"\n",
    "    result = client.search(collection_name=\"pdf_collection\", query_vector=embeddings_model.embed_documents([input])[0], limit=10)\n",
    "    return [{'text': r.payload['text'], 'pdf_path': r.payload['pdf_file'], 'page_number': r.payload['page'], 'idx': r.payload['idx'], 'score': r.score} for r in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You are a agent that is going to be searching technical research papers. You will be given a topic to find information on. You will need to find the most relevant information and return back the information with sources. \n",
    "        Make sure you only return information that you have retrieved from the research papers. Do not make up any information. You will have access to tools to help you find the information.\n",
    "\n",
    "        Make sure to make uses of the tools provided. You can use the tools as many times as needed. Make sure you have all the information needed before returning the information.\n",
    "\n",
    "        Return the output by stating the information you found and the source of the information in the format pdf_path - page number - the section (if mentioned) in the papers we queried. The page number and pdf_path should only be from our vector store, do not make up any page numbers or pdf paths.\n",
    "\n",
    "        eg: \n",
    "        // brief summary of the information only based on the tools we used\n",
    "        // pdf_path \n",
    "        // page number\n",
    "        // section (if it can be found in the text of the result)\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\"human\", \"Research {research_topic}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    max_retries=2,\n",
    "    temperature=0.1,\n",
    ")\n",
    "agent_tools = [natural_language_query]\n",
    "llm = llm.bind_tools(agent_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_scratchpad_formatter(x):\n",
    "    return format_to_openai_tool_messages(x[\"intermediate_steps\"])\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"research_topic\": lambda x: x[\"research_topic\"],\n",
    "        \"agent_scratchpad\": agent_scratchpad_formatter,\n",
    "    }\n",
    "    | research_prompt\n",
    "    | llm\n",
    "    | OpenAIToolsAgentOutputParser() # This gets the output from the OpenAI api call and gets the tool invocations from it. This then is executed by the AgentExecutor.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=agent_tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `natural_language_query` with `{'input': 'Where is the data from the map function stored?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'text': 'The master is the conduit through which the location of interme- diate file regions is propagated from map tasks to reduce tasks. There - fore, for each completed map task, the master stores the locations and sizes of the R intermediate file regions produced by the map task. Updates to this location and size information are received as map tasks are completed. The information is pushed incrementally to workers that have in-progress reduce tasks.', 'pdf_path': '/Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf', 'page_number': '3', 'idx': '15.txt', 'score': 0.41727197}, {'text': '2. One of the copies of the program—the master— is special. The rest are workers that are assigned work by the master. There are M map tasks and R reduce tasks to assign. The master picks idle workers and assigns each one a map task or a reduce task.\\n\\n3. A worker who is assigned a map task reads the contents of the corre- sponding input split. It parses key/value pairs out of the input data and passes each pair to the user-defined map function. The intermediate key/value pairs produced by the map function are buffered in memory.\\n\\n4. Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master who is responsible for forwarding these locations to the reduce workers.', 'pdf_path': '/Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf', 'page_number': '2', 'idx': '11.txt', 'score': 0.40317342}, {'text': 'Fig. 2. Data transfer rate over time (mr-grep).\\n\\nFigure 2 shows the progress of the computation over time. The Y-axis shows the rate at which the input data is scanned. The rate grad- ually picks up as more machines are assigned to this MapReduce com- putation and peaks at over 30 GB/s when 1764 workers have been assigned. As the map tasks finish, the rate starts dropping and hits zero about 80 seconds into the computation. The entire computation takes approximately 150 seconds from start to finish. This includes about a minute of startup overhead. The overhead is due to the propagation of the program to all worker machines and delays interacting with GFS to open the set of 1000 input files and to get the information needed for the locality optimization.\\n\\n5.3 Sort The sort program sorts 1010 100-byte records (approximately 1 terabyte of data). This program is modeled after the TeraSort benchmark [12].\\n\\nMapReduce: Simplified Data Processing on Large Clusters', 'pdf_path': '/Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf', 'page_number': '5', 'idx': '30.txt', 'score': 0.37074327}, {'text': 'Handling Worker Failures The master pings every worker periodically. If no response is received from a worker in a certain amount of time, the master marks the worker as failed. Any map tasks completed by the worker are reset back to their initial idle state and therefore become eligible for scheduling on other workers. Similarly, any map task or reduce task in progress on a failed worker is also reset to idle and becomes eligible for rescheduling.\\n\\nCompleted map tasks are reexecuted on a failure because their out- put is stored on the local disk(s) of the failed machine and is therefore inaccessible. Completed reduce tasks do not need to be reexecuted since their output is stored in a global file system.\\n\\n3.2 Master Data Structures The master keeps several data structures. For each map task and reduce task, it stores the state (idle, in-progress, or completed) and the identity of the worker machine (for nonidle tasks).', 'pdf_path': '/Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf', 'page_number': '3', 'idx': '14.txt', 'score': 0.35900104}, {'text': 'computations we were trying to perform but hides the messy details of parallelization, fault tolerance, data distri- bution and load balancing in a library. Our abstraction is inspired by the map and reduce primitives present in Lisp and many other functional lan- guages. We realized that most of our computations involved applying a map operation to each logical record’ in our input in order to compute a set of intermediate key/value pairs, and then applying a reduce operation to all the values that shared the same key in order to combine the derived data appropriately. Our use of a functional model with user-specified map and reduce operations allows us to parallelize large computations easily and to use reexecution as the primary mechanism for fault tolerance.', 'pdf_path': '/Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf', 'page_number': '1', 'idx': '2.txt', 'score': 0.348928}, {'text': 'Emit(AsString(result));\\n\\nThe map function emits each word plus an associated count of occurrences (just 1 in this simple example). The reduce function sums together all counts emitted for a particular word.\\n\\nIn addition, the user writes code to fill in a mapreduce specification object with the names of the input and output files and optional tun- ing parameters. The user then invokes the MapReduce function, pass- ing it to the specification object. The user’s code is linked together with the MapReduce library (implemented in C++). Our original MapReduce paper contains the full program text for this example [8]. More than ten thousand distinct programs have been implemented using MapReduce at Google, including algorithms for large-scale graph processing, text processing, data mining, machine learning, sta- tistical machine translation, and many other areas. More discussion of specific applications of MapReduce can be found elsewhere [8, 16, 7].', 'pdf_path': '/Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf', 'page_number': '2', 'idx': '6.txt', 'score': 0.3484044}, {'text': '3.1 Execution Overview The map invocations are distributed across multiple machines by auto- matically partitioning the input data into a set of M splits. The input splits can be processed in parallel by different machines. Reduce invo- cations are distributed by partitioning the intermediate key space into R pieces using a partitioning function (e.g., hash(key) mod R). The number of partitions (R) and the partitioning function are specified by the user. Figure 1 shows the overall flow of a MapReduce operation in our implementation. When the user program calls the MapReduce func- tion, the following sequence of actions occurs (the numbered labels in Figure 1 correspond to the numbers in the following list).\\n\\n1. The MapReduce library in the user program first splits the input files into M pieces of typically 16-64MB per piece (controllable by the user via an optional parameter). It then starts up many copies of the program on a cluster of machines.', 'pdf_path': '/Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf', 'page_number': '2', 'idx': '10.txt', 'score': 0.34698963}, {'text': '1 Introduction Prior to our development of MapReduce, the authors and many others at Google implemented hundreds of special-purpose computations that process large amounts of raw data, such as crawled documents, Web request logs, etc., to compute various kinds of derived data, such as inverted indices, various representations of the graph structure of Web documents, summaries of the number of pages crawled per host, and the set of most frequent queries in a given day. Most such computa- tions are conceptually straightforward. However, the input data is usu- ally large and the computations have to be distributed across hundreds or thousands of machines in order to finish in a reasonable amount of time. The issues of how to parallelize the computation, distribute the data, and handle failures conspire to obscure the original simple com- putation with large amounts of complex code to deal with these issues. As a reaction to this complexity, we designed a new abstraction that allows us to express the simple', 'pdf_path': '/Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf', 'page_number': '1', 'idx': '1.txt', 'score': 0.3414455}, {'text': '3.4 Locality Network bandwidth is a relatively scarce resource in our computing envi- ronment. We conserve network bandwidth by taking advantage of the fact that the input data (managed by GFS [10]) is stored on the local disks of the machines that make up our cluster. GFS divides each file into 64MB blocks and stores several copies of each block (typically 3 copies) on different machines. The MapReduce master takes the loca- tion information of the input files into account and attempts to schedule a map task on a machine that contains a replica of the corresponding input data. Failing that, it attempts to schedule a map task near a replica of that task’s input data (e.g., on a worker machine that is on the same network switch as the machine containing the data). When running large MapReduce operations on a significant fraction of the workers in a clus- ter, most input data is read locally and consumes no network bandwidth.', 'pdf_path': '/Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf', 'page_number': '4', 'idx': '21.txt', 'score': 0.33965173}, {'text': 'Semantics in the Presence of Failures When the user-supplied map and reduce operators are deterministic functions of their input values, our distributed implementation pro- duces the same output as would have been produced by a nonfaulting sequential execution of the entire program.\\n\\nCOMMUNICATIONS OF THE ACM January 2008/Vol. 51, No. 1 109\\n\\nWe rely on atomic commits of map and reduce task outputs to achieve this property. Each in-progress task writes its output to private temporary files. A reduce task produces one such file, and a map task produces R such files (one per reduce task). When a map task com- pletes, the worker sends a message to the master and includes the names of the R temporary files in the message. If the master receives a comple- tion message for an already completed map task, it ignores the message. Otherwise, it records the names of R files in a master data structure.', 'pdf_path': '/Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf', 'page_number': '3', 'idx': '17.txt', 'score': 0.3290829}]\u001b[0m\u001b[32;1m\u001b[1;3mThe data from the map function is initially buffered in memory and then periodically written to the local disk. It is partitioned into regions by a partitioning function, and the locations of these buffered pairs on the local disk are passed back to the master. The master is responsible for forwarding these locations to the reduce workers. \n",
      "\n",
      "- **pdf_path**: /Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf\n",
      "- **page number**: 2\n",
      "\n",
      "Additionally, for each completed map task, the master stores the locations and sizes of the intermediate file regions produced by the map task. This information is pushed incrementally to workers that have in-progress reduce tasks.\n",
      "\n",
      "- **pdf_path**: /Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf\n",
      "- **page number**: 3\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'research_topic': 'Where is the data from the map function stored?',\n",
       " 'output': 'The data from the map function is initially buffered in memory and then periodically written to the local disk. It is partitioned into regions by a partitioning function, and the locations of these buffered pairs on the local disk are passed back to the master. The master is responsible for forwarding these locations to the reduce workers. \\n\\n- **pdf_path**: /Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf\\n- **page number**: 2\\n\\nAdditionally, for each completed map task, the master stores the locations and sizes of the intermediate file regions produced by the map task. This information is pushed incrementally to workers that have in-progress reduce tasks.\\n\\n- **pdf_path**: /Users/faizahmed/Documents/SJSU/topics_in_db/Project/data-ingestion/data/mapreduce.pdf\\n- **page number**: 3'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_agent_result = await agent_executor.ainvoke({\"research_topic\": \"Where is the data from the map function stored?\"})\n",
    "research_agent_result['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
